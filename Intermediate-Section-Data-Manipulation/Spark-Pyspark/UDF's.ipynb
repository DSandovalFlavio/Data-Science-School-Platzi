{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, StringType, Row\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import col, when, lit"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "spark = SparkContext(master=\"local\", appName=\"DataFrame\")\n",
    "sqlContext = SQLContext(spark)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "21/09/26 19:46:47 WARN Utils: Your hostname, DESKTOP-NVKHLIO resolves to a loopback address: 127.0.1.1; using 192.168.210.162 instead (on interface eth0)\n",
      "21/09/26 19:46:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "21/09/26 19:46:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "spark.stop()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "!head -n 5 ./files/deportistaError.csv"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "deportista_id,nombre,genero,edad,altura,peso,equipo_id\n",
      "1,A Dijiang,1,24,180,80,199\n",
      "2,A Lamusi,1,23,170,60,199\n",
      "3,Gunnar Nielsen Aaby,1,24,,,273\n",
      "4,Edgar Lindenau Aabye,1,34,,,278\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "deportistaError = spark.textFile(\"./files/deportistaError.csv\")\\\n",
    "    .map(lambda line: line.split(\",\"))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def eliminaEncabezado(indice,iterador):\n",
    "    return iter(list(iterador)[1:])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "deportistaError = deportistaError.mapPartitionsWithIndex(eliminaEncabezado)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "deportistaError.take(3)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['1', 'A Dijiang', '1', '24', '180', '80', '199'],\n",
       " ['2', 'A Lamusi', '1', '23', '170', '60', '199'],\n",
       " ['3', 'Gunnar Nielsen Aaby', '1', '24', '', '', '273']]"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "deportistaError.map(lambda line: (\n",
    "    line[0],\n",
    "    line[1],\n",
    "    line[2],\n",
    "    line[3],\n",
    "    line[4],\n",
    "    line[5],\n",
    "    line[6]))\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"deportista_id\", StringType(), False),\n",
    "    StructField(\"nombre\", StringType(), False),\n",
    "    StructField(\"genero\", StringType(), False),\n",
    "    StructField(\"edad\", StringType(), False),\n",
    "    StructField(\"altura\", StringType(), False),\n",
    "    StructField(\"peso\", StringType(), False),\n",
    "    StructField(\"equipo_id\", StringType(), False)\n",
    "])\n",
    "\n",
    "deportistaErrorDF = sqlContext.createDataFrame(deportistaError, schema)\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "deportistaErrorDF.show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------------+--------------------+------+----+------+----+---------+\n",
      "|deportista_id|              nombre|genero|edad|altura|peso|equipo_id|\n",
      "+-------------+--------------------+------+----+------+----+---------+\n",
      "|            1|           A Dijiang|     1|  24|   180|  80|      199|\n",
      "|            2|            A Lamusi|     1|  23|   170|  60|      199|\n",
      "|            3| Gunnar Nielsen Aaby|     1|  24|      |    |      273|\n",
      "|            4|Edgar Lindenau Aabye|     1|  34|      |    |      278|\n",
      "|            5|Christine Jacoba ...|     2|  21|   185|  82|      705|\n",
      "|            6|     Per Knut Aaland|     1|  31|   188|  75|     1096|\n",
      "|            7|        John Aalberg|     1|  31|   183|  72|     1096|\n",
      "|            8|\"Cornelia \"\"Cor\"\"...|     2|  18|   168|    |      705|\n",
      "|            9|    Antti Sami Aalto|     1|  26|   186|  96|      350|\n",
      "|           10|\"Einar Ferdinand ...|     1|  26|      |    |      350|\n",
      "|           11|  Jorma Ilmari Aalto|     1|  22|   182|76.5|      350|\n",
      "|           12|   Jyri Tapani Aalto|     1|  31|   172|  70|      350|\n",
      "|           13|  Minna Maarit Aalto|     2|  30|   159|55.5|      350|\n",
      "|           14|Pirjo Hannele Aal...|     2|  32|   171|  65|      350|\n",
      "|           15|Arvo Ossian Aaltonen|     1|  22|      |    |      350|\n",
      "|           16|Juhamatti Tapio A...|     1|  28|   184|  85|      350|\n",
      "|           17|Paavo Johannes Aa...|     1|  28|   175|  64|      350|\n",
      "|           18|Timo Antero Aaltonen|     1|  31|   189| 130|      350|\n",
      "|           19|Win Valdemar Aalt...|     1|  54|      |    |      350|\n",
      "|           20|  Kjetil Andr Aamodt|     1|  20|   176|  85|      742|\n",
      "+-------------+--------------------+------+----+------+----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def conversionEnteros(valor):\n",
    "    try:\n",
    "        return int(valor)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "conversionEnterosUDF = udf(lambda z: conversionEnteros(z), IntegerType())\n",
    "sqlContext.udf.register(\"conversionEnterosUDF\", conversionEnterosUDF)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "21/09/26 19:57:02 WARN SimpleFunctionRegistry: The function conversionenterosudf replaced a previously registered function.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(z)>"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "deportistaErrorDF.select(conversionEnterosUDF(\"altura\").alias(\"altura_int\")).show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "21/09/26 19:57:05 ERROR PythonUDFRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/dsandovalflavio/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 584, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/home/dsandovalflavio/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:84)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:67)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/dsandovalflavio/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n",
      "    process()\n",
      "  File \"/home/dsandovalflavio/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/dsandovalflavio/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/dsandovalflavio/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/dsandovalflavio/spark/python/pyspark/sql/session.py\", line 682, in prepare\n",
      "    verify_func(obj)\n",
      "  File \"/home/dsandovalflavio/spark/python/pyspark/sql/types.py\", line 1409, in verify\n",
      "    verify_value(obj)\n",
      "  File \"/home/dsandovalflavio/spark/python/pyspark/sql/types.py\", line 1388, in verify_struct\n",
      "    \"length of fields (%d)\" % (len(obj), len(verifiers))))\n",
      "ValueError: Length of object (8) does not match with length of fields (7)\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1158)\n",
      "\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1174)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1212)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:53)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:397)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1996)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:232)\n",
      "21/09/26 19:57:05 ERROR PythonUDFRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/dsandovalflavio/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n",
      "    process()\n",
      "  File \"/home/dsandovalflavio/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/dsandovalflavio/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/dsandovalflavio/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/dsandovalflavio/spark/python/pyspark/sql/session.py\", line 682, in prepare\n",
      "    verify_func(obj)\n",
      "  File \"/home/dsandovalflavio/spark/python/pyspark/sql/types.py\", line 1409, in verify\n",
      "    verify_value(obj)\n",
      "  File \"/home/dsandovalflavio/spark/python/pyspark/sql/types.py\", line 1388, in verify_struct\n",
      "    \"length of fields (%d)\" % (len(obj), len(verifiers))))\n",
      "ValueError: Length of object (8) does not match with length of fields (7)\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1158)\n",
      "\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1174)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1212)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:53)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:397)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1996)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:232)\n",
      "21/09/26 19:57:05 ERROR Executor: Exception in task 0.0 in stage 5.0 (TID 5)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/dsandovalflavio/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n",
      "    process()\n",
      "  File \"/home/dsandovalflavio/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/dsandovalflavio/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/dsandovalflavio/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/dsandovalflavio/spark/python/pyspark/sql/session.py\", line 682, in prepare\n",
      "    verify_func(obj)\n",
      "  File \"/home/dsandovalflavio/spark/python/pyspark/sql/types.py\", line 1409, in verify\n",
      "    verify_value(obj)\n",
      "  File \"/home/dsandovalflavio/spark/python/pyspark/sql/types.py\", line 1388, in verify_struct\n",
      "    \"length of fields (%d)\" % (len(obj), len(verifiers))))\n",
      "ValueError: Length of object (8) does not match with length of fields (7)\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1158)\n",
      "\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1174)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1212)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:53)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:397)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1996)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:232)\n",
      "21/09/26 19:57:05 WARN TaskSetManager: Lost task 0.0 in stage 5.0 (TID 5) (192.168.210.162 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/dsandovalflavio/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n",
      "    process()\n",
      "  File \"/home/dsandovalflavio/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/dsandovalflavio/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/dsandovalflavio/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/dsandovalflavio/spark/python/pyspark/sql/session.py\", line 682, in prepare\n",
      "    verify_func(obj)\n",
      "  File \"/home/dsandovalflavio/spark/python/pyspark/sql/types.py\", line 1409, in verify\n",
      "    verify_value(obj)\n",
      "  File \"/home/dsandovalflavio/spark/python/pyspark/sql/types.py\", line 1388, in verify_struct\n",
      "    \"length of fields (%d)\" % (len(obj), len(verifiers))))\n",
      "ValueError: Length of object (8) does not match with length of fields (7)\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1158)\n",
      "\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1174)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1212)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:53)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:397)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1996)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:232)\n",
      "\n",
      "21/09/26 19:57:05 ERROR TaskSetManager: Task 0 in stage 5.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/home/dsandovalflavio/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/home/dsandovalflavio/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/dsandovalflavio/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/dsandovalflavio/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/dsandovalflavio/spark/python/pyspark/sql/session.py\", line 682, in prepare\n    verify_func(obj)\n  File \"/home/dsandovalflavio/spark/python/pyspark/sql/types.py\", line 1409, in verify\n    verify_value(obj)\n  File \"/home/dsandovalflavio/spark/python/pyspark/sql/types.py\", line 1388, in verify_struct\n    \"length of fields (%d)\" % (len(obj), len(verifiers))))\nValueError: Length of object (8) does not match with length of fields (7)\n",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2990/11454766.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdeportistaErrorDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconversionEnterosUDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"altura\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"altura_int\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \"\"\"\n\u001b[1;32m    483\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pyspark/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1310\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/home/dsandovalflavio/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/home/dsandovalflavio/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/dsandovalflavio/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/dsandovalflavio/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/dsandovalflavio/spark/python/pyspark/sql/session.py\", line 682, in prepare\n    verify_func(obj)\n  File \"/home/dsandovalflavio/spark/python/pyspark/sql/types.py\", line 1409, in verify\n    verify_value(obj)\n  File \"/home/dsandovalflavio/spark/python/pyspark/sql/types.py\", line 1388, in verify_struct\n    \"length of fields (%d)\" % (len(obj), len(verifiers))))\nValueError: Length of object (8) does not match with length of fields (7)\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.11 64-bit ('pyspark': conda)"
  },
  "interpreter": {
   "hash": "ba24e5d53ea9beca6a5a46cb4f0dc3de1728e14498529be8f7da7453563971da"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}